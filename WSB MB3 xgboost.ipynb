{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-011113936377\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://{bucket}/wsb/data/ data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc.lower()) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t) and t.upper() not in allsymbols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET = pytz.timezone('US/Eastern')\n",
    "\n",
    "ignorelist=[\"DD\",\"FREE\",\"CASH\",\"ON\",\"I\"]\n",
    "allsymbols=[]\n",
    "with open(\"allsymbols.txt\") as fh:\n",
    "    allsymbols=fh.readlines()\n",
    "allsymbols = [x.strip() for x in allsymbols]\n",
    "\n",
    "dt=datetime.timedelta(days=7)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "stockdata={}\n",
    "def getstockprice(symbol,date):\n",
    "    if symbol not in stockdata:\n",
    "        with open(\"stockprices/\"+symbol,\"r\") as fh:\n",
    "            data=json.load(fh)\n",
    "        stockdata[symbol]={}\n",
    "        for stockday in data:\n",
    "            indate=datetime.datetime.strptime(stockday[\"begins_at\"],\"%Y-%m-%dT%H:%M:%SZ\").date()\n",
    "            stockdata[symbol][indate]=stockday\n",
    "    today=datetime.date.today()\n",
    "    while True:\n",
    "        #print(date)\n",
    "        if date in stockdata[symbol]:\n",
    "            return float(stockdata[symbol][date]['close_price'])\n",
    "        date=date+datetime.timedelta(days=1)\n",
    "        if date>today:\n",
    "            break\n",
    "    raise Exception(\"No stock data found\")\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 99.9%\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"data/\")\n",
    "\n",
    "foundsymbols=[]\n",
    "relevant_posts = []\n",
    "i=0\n",
    "for file in files:\n",
    "  if(round(i/100,0)==i/100):\n",
    "        update_progress(i / len(files))  \n",
    "  with open(\"data/\"+file,\"r\") as fh:\n",
    "    try:\n",
    "        data=json.load(fh)\n",
    "        text = (data[\"title\"]+\" \"+data[\"selftext\"]).replace('\\\"','').replace('\\'','')\n",
    "        allmatches=[]\n",
    "        matches=re.findall('\\W*([A-Z][A-Z\\.]{0,3})\\W',text)\n",
    "        for submatch in matches:\n",
    "            allmatches.append({submatch:submatch})\n",
    "            #pass\n",
    "        matches2=re.findall('\\W*(\\$[a-z\\.]{1,4})',text)\n",
    "        for submatch in matches2:\n",
    "            allmatches.append({submatch.upper()[1:]:submatch})\n",
    "        thesesymbols=[]\n",
    "        #print(allmatches)\n",
    "        for submatch in allmatches:\n",
    "            symbol=list(submatch.keys())[0]\n",
    "            #print(submatch.keys())\n",
    "            #print(symbol)\n",
    "            #print([list(x.keys())[0] for x in thesesymbols])\n",
    "            if symbol in allsymbols and symbol not in [list(x.keys())[0] for x in thesesymbols] and symbol not in ignorelist:\n",
    "                thesesymbols.append(submatch)\n",
    "                if symbol not in foundsymbols:\n",
    "                    foundsymbols.append(symbol)\n",
    "        if len(thesesymbols)>0:\n",
    "            relevant_posts.append({\"data\":data,\"symbols\":thesesymbols})\n",
    "            #print(thesesymbols)\n",
    "    except Exception as err:\n",
    "      print(\"Error with\",file)\n",
    "      raise(err)\n",
    "  i+=1\n",
    "    \n",
    "\n",
    "  #if i>1000:\n",
    "  #  break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TSLA',\n",
       " 'B',\n",
       " 'AMZN',\n",
       " 'COST',\n",
       " 'C',\n",
       " 'LULU',\n",
       " 'AMD',\n",
       " 'FB',\n",
       " 'PTON',\n",
       " 'AAPL',\n",
       " 'GOLD',\n",
       " 'ROKU',\n",
       " 'NFLX',\n",
       " 'DIS',\n",
       " 'HAS',\n",
       " 'GOOD',\n",
       " 'ONCY',\n",
       " 'SPCE',\n",
       " 'AMC',\n",
       " 'KR',\n",
       " 'PM',\n",
       " 'ABC',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'CYBR',\n",
       " 'PANW',\n",
       " 'RPD',\n",
       " 'EDIT',\n",
       " 'DTE',\n",
       " 'DCOM',\n",
       " 'AT',\n",
       " 'ACB',\n",
       " 'THC',\n",
       " 'MU',\n",
       " 'IP',\n",
       " 'AG',\n",
       " 'GILD',\n",
       " 'TJX',\n",
       " 'HTZ',\n",
       " 'TVTY',\n",
       " 'DGX',\n",
       " 'QQQ',\n",
       " 'TA',\n",
       " 'BYND',\n",
       " 'POST',\n",
       " 'SR',\n",
       " 'NIO',\n",
       " 'RING',\n",
       " 'A',\n",
       " 'NEW',\n",
       " 'IT',\n",
       " 'FOSL',\n",
       " 'BK',\n",
       " 'PINS',\n",
       " 'KRTX',\n",
       " 'OVID',\n",
       " 'DOW',\n",
       " 'SBUX',\n",
       " 'VC',\n",
       " 'MCD',\n",
       " 'MSFT',\n",
       " 'WMT',\n",
       " 'SNAP',\n",
       " 'PECK',\n",
       " 'HUGE',\n",
       " 'GDP',\n",
       " 'WELL',\n",
       " 'CDC',\n",
       " 'BA',\n",
       " 'V',\n",
       " 'FAT',\n",
       " 'NOC',\n",
       " 'LMT',\n",
       " 'RH',\n",
       " 'AAL',\n",
       " 'GO',\n",
       " 'SDC',\n",
       " 'ALGN',\n",
       " 'ONE',\n",
       " 'ATVI',\n",
       " 'M',\n",
       " 'CEO',\n",
       " 'K',\n",
       " 'ALLY',\n",
       " 'SIX',\n",
       " 'CME',\n",
       " 'AM',\n",
       " 'LIVE',\n",
       " 'FIT',\n",
       " 'SWBI',\n",
       " 'EC',\n",
       " 'F',\n",
       " 'CRM',\n",
       " 'BAC',\n",
       " 'CTO',\n",
       " 'PEP',\n",
       " 'UAL',\n",
       " 'DAL',\n",
       " 'JNJ',\n",
       " 'PCG',\n",
       " 'E',\n",
       " 'PLUS',\n",
       " 'AMTD',\n",
       " 'TTD',\n",
       " 'DEA',\n",
       " 'CBD',\n",
       " 'CCL',\n",
       " 'ARE',\n",
       " 'OUT',\n",
       " 'G',\n",
       " 'BMO',\n",
       " 'IONS',\n",
       " 'FOR',\n",
       " 'HAL',\n",
       " 'INFY',\n",
       " 'KO',\n",
       " 'SYF',\n",
       " 'NDAQ',\n",
       " 'NEE',\n",
       " 'ERIC',\n",
       " 'DPZ',\n",
       " 'LUV',\n",
       " 'UNP',\n",
       " 'AXP',\n",
       " 'SNY',\n",
       " 'NEXT',\n",
       " 'MMM',\n",
       " 'UPS',\n",
       " 'CAT',\n",
       " 'TRU',\n",
       " 'HOG',\n",
       " 'APPS',\n",
       " 'ELLO',\n",
       " 'LOAN',\n",
       " 'VZ',\n",
       " 'APHA',\n",
       " 'R',\n",
       " 'SO',\n",
       " 'MGM',\n",
       " 'T',\n",
       " 'ROCK',\n",
       " 'SSSS',\n",
       " 'LAND',\n",
       " 'AI',\n",
       " 'III',\n",
       " 'PLUG',\n",
       " 'EOD',\n",
       " 'ULTA',\n",
       " 'GM',\n",
       " 'BBBY',\n",
       " 'CPRX',\n",
       " 'TD',\n",
       " 'CLX',\n",
       " 'TH',\n",
       " 'ARCH',\n",
       " 'BE',\n",
       " 'KHC',\n",
       " 'PG',\n",
       " 'SHOP',\n",
       " 'TQQQ',\n",
       " 'ETSY',\n",
       " 'TSM',\n",
       " 'BSX',\n",
       " 'PE',\n",
       " 'GF',\n",
       " 'FCEL',\n",
       " 'NVDA',\n",
       " 'GPS',\n",
       " 'FT',\n",
       " 'GME',\n",
       " 'BIG',\n",
       " 'HPQ',\n",
       " 'HP',\n",
       " 'UBER',\n",
       " 'BY',\n",
       " 'ANY',\n",
       " 'ALKS',\n",
       " 'ALL',\n",
       " 'NOW',\n",
       " 'STAY',\n",
       " 'CFA',\n",
       " 'BABA',\n",
       " 'ACAD',\n",
       " 'KSS',\n",
       " 'CROX',\n",
       " 'IBM',\n",
       " 'IR',\n",
       " 'JP',\n",
       " 'DFS',\n",
       " 'LL',\n",
       " 'DB',\n",
       " 'SQ',\n",
       " 'MTCH',\n",
       " 'ATH',\n",
       " 'USA',\n",
       " 'OR',\n",
       " 'SELF',\n",
       " 'INO',\n",
       " 'TWO',\n",
       " 'UI',\n",
       " 'NMI',\n",
       " 'NMIH',\n",
       " 'WTI',\n",
       " 'CL',\n",
       " 'IOVA',\n",
       " 'LN',\n",
       " 'L',\n",
       " 'PD',\n",
       " 'CAR',\n",
       " 'SMFG',\n",
       " 'ZM',\n",
       " 'JPM',\n",
       " 'CFO',\n",
       " 'EVER',\n",
       " 'KL',\n",
       " 'WPM',\n",
       " 'SSRM',\n",
       " 'GEO',\n",
       " 'XLNX',\n",
       " 'EAST',\n",
       " 'CSCO',\n",
       " 'PSA',\n",
       " 'TWTR',\n",
       " 'ENPH',\n",
       " 'ALLT',\n",
       " 'GAIN',\n",
       " 'O',\n",
       " 'D',\n",
       " 'H',\n",
       " 'ZNGA',\n",
       " 'AUPH',\n",
       " 'PCIM',\n",
       " 'PS',\n",
       " 'GNUS',\n",
       " 'DBX',\n",
       " 'HSBC',\n",
       " 'UNIT',\n",
       " 'AIMT',\n",
       " 'BBY',\n",
       " 'LYFT',\n",
       " 'GOOG',\n",
       " 'OXY',\n",
       " 'IMO',\n",
       " 'DHT',\n",
       " 'TNK',\n",
       " 'FRO',\n",
       " 'NAT',\n",
       " 'SA',\n",
       " 'TV',\n",
       " 'DVD',\n",
       " 'RGR',\n",
       " 'RAD',\n",
       " 'J',\n",
       " 'AYI',\n",
       " 'KB',\n",
       " 'KBH',\n",
       " 'WD',\n",
       " 'WDFC',\n",
       " 'ADES',\n",
       " 'XP',\n",
       " 'AGTC',\n",
       " 'AMAG',\n",
       " 'SMPL',\n",
       " 'LB',\n",
       " 'AZZ',\n",
       " 'XRX',\n",
       " 'VOYA',\n",
       " 'BLK',\n",
       " 'PDT',\n",
       " 'BAM',\n",
       " 'KTOS',\n",
       " 'IQ',\n",
       " 'TEVA',\n",
       " 'MO',\n",
       " 'BUD',\n",
       " 'PRT',\n",
       " 'ES',\n",
       " 'WORK',\n",
       " 'CMG',\n",
       " 'PPT',\n",
       " 'BRO',\n",
       " 'APA',\n",
       " 'DXC',\n",
       " 'DISH',\n",
       " 'SNR',\n",
       " 'FIVE',\n",
       " 'NI',\n",
       " 'KMI',\n",
       " 'OSG',\n",
       " 'STNG',\n",
       " 'EURN',\n",
       " 'OSTK',\n",
       " 'EA',\n",
       " 'TS',\n",
       " 'YY',\n",
       " 'MAR',\n",
       " 'PAR',\n",
       " 'ET',\n",
       " 'TLRY',\n",
       " 'SQQQ',\n",
       " 'ESNT',\n",
       " 'PLC',\n",
       " 'UBS',\n",
       " 'ALLK',\n",
       " 'LOGI',\n",
       " 'BIDU',\n",
       " 'ANET',\n",
       " 'CRWD',\n",
       " 'CAN',\n",
       " 'PZZA',\n",
       " 'WEN',\n",
       " 'HEAR',\n",
       " 'JACK',\n",
       " 'Z',\n",
       " 'ZS',\n",
       " 'ABIO',\n",
       " 'TGT',\n",
       " 'DKNG',\n",
       " 'MS',\n",
       " 'REAL',\n",
       " 'WOW',\n",
       " 'CGC',\n",
       " 'FSLY',\n",
       " 'CTL',\n",
       " 'ICD',\n",
       " 'PNC',\n",
       " 'APRN',\n",
       " 'TDA',\n",
       " 'SHAK',\n",
       " 'HD',\n",
       " 'WKHS',\n",
       " 'VCEL',\n",
       " 'MDWD',\n",
       " 'VFF',\n",
       " 'DOCU',\n",
       " 'EB',\n",
       " 'TCOM',\n",
       " 'XOM',\n",
       " 'AAOI',\n",
       " 'RCL',\n",
       " 'PLNT',\n",
       " 'MSB',\n",
       " 'W',\n",
       " 'PHAT',\n",
       " 'GE',\n",
       " 'STZ',\n",
       " 'FDX',\n",
       " 'EV',\n",
       " 'GL',\n",
       " 'FL',\n",
       " 'SRPT',\n",
       " 'SLDB',\n",
       " 'STMP',\n",
       " 'NTNX',\n",
       " 'SRNE',\n",
       " 'PHD',\n",
       " 'SCI',\n",
       " 'TTWO',\n",
       " 'NHC',\n",
       " 'SE',\n",
       " 'RNG',\n",
       " 'LOCO',\n",
       " 'DLTR',\n",
       " 'AIG',\n",
       " 'TAL',\n",
       " 'DE',\n",
       " 'DXCM',\n",
       " 'APT',\n",
       " 'NKLA',\n",
       " 'FCF',\n",
       " 'HOME',\n",
       " 'TELL',\n",
       " 'TLT',\n",
       " 'MIND',\n",
       " 'CNA',\n",
       " 'WINS',\n",
       " 'TREE',\n",
       " 'MTD',\n",
       " 'KRP',\n",
       " 'WFC',\n",
       " 'GS',\n",
       " 'UNH',\n",
       " 'INTC',\n",
       " 'BIIB',\n",
       " 'HCA',\n",
       " 'BOH',\n",
       " 'MTB',\n",
       " 'ONB',\n",
       " 'SAP',\n",
       " 'BMRC',\n",
       " 'CMA',\n",
       " 'PHG',\n",
       " 'CBU',\n",
       " 'KMB',\n",
       " 'TFC',\n",
       " 'PLD',\n",
       " 'LII',\n",
       " 'LLY',\n",
       " 'SNA',\n",
       " 'LVS',\n",
       " 'TRV',\n",
       " 'LRCX',\n",
       " 'KALU',\n",
       " 'EMR',\n",
       " 'CIT',\n",
       " 'FITB',\n",
       " 'BG',\n",
       " 'VIR',\n",
       " 'MRNA',\n",
       " 'CODX',\n",
       " 'APEX',\n",
       " 'BB',\n",
       " 'CARV',\n",
       " 'MOMO',\n",
       " 'NVO',\n",
       " 'LNG',\n",
       " 'USAC',\n",
       " 'DAX',\n",
       " 'EARS',\n",
       " 'CZR',\n",
       " 'AN',\n",
       " 'EYE',\n",
       " 'MA',\n",
       " 'ATR',\n",
       " 'GT',\n",
       " 'MRK',\n",
       " 'PT',\n",
       " 'KN',\n",
       " 'JCO',\n",
       " 'UTI',\n",
       " 'CVM',\n",
       " 'AZN',\n",
       " 'GD',\n",
       " 'SGA',\n",
       " 'TTNP',\n",
       " 'FLAT',\n",
       " 'NG',\n",
       " 'MTNB',\n",
       " 'ALGT',\n",
       " 'IRL',\n",
       " 'ARDS',\n",
       " 'WASH',\n",
       " 'WH',\n",
       " 'CO',\n",
       " 'FOX',\n",
       " 'LE',\n",
       " 'VEEV',\n",
       " 'SAFE',\n",
       " 'ISRG',\n",
       " 'TDOC',\n",
       " 'ST',\n",
       " 'WIX',\n",
       " 'FCX',\n",
       " 'CUZ',\n",
       " 'ASRT',\n",
       " 'VAC',\n",
       " 'TMUS',\n",
       " 'VSTO',\n",
       " 'SPR',\n",
       " 'RMED',\n",
       " 'OLN',\n",
       " 'UXIN',\n",
       " 'KEYS',\n",
       " 'MASI',\n",
       " 'RTX',\n",
       " 'CHGG',\n",
       " 'SP',\n",
       " 'IRS',\n",
       " 'CSX',\n",
       " 'BBW',\n",
       " 'ING',\n",
       " 'RVLV',\n",
       " 'CAC',\n",
       " 'MAN',\n",
       " 'BR',\n",
       " 'THO',\n",
       " 'AVGO',\n",
       " 'HRB',\n",
       " 'PLAY',\n",
       " 'HDS',\n",
       " 'LOVE',\n",
       " 'CHS',\n",
       " 'TLRD',\n",
       " 'SECO',\n",
       " 'DLTH',\n",
       " 'SHLO',\n",
       " 'ASNA',\n",
       " 'CPST',\n",
       " 'JW.A',\n",
       " 'AZRE',\n",
       " 'ADXS',\n",
       " 'CASY',\n",
       " 'LMNR',\n",
       " 'BBCP',\n",
       " 'NEPT',\n",
       " 'OXM',\n",
       " 'SPCB',\n",
       " 'TMDX',\n",
       " 'TUFN',\n",
       " 'LIVX',\n",
       " 'RENN',\n",
       " 'SMMT',\n",
       " 'CRWS',\n",
       " 'UROV',\n",
       " 'CHWY',\n",
       " 'CAKE',\n",
       " 'API',\n",
       " 'FSD',\n",
       " 'ACA',\n",
       " 'CI',\n",
       " 'ANTM',\n",
       " 'HUM',\n",
       " 'MOH',\n",
       " 'CNC',\n",
       " 'NCLH',\n",
       " 'TWNK',\n",
       " 'CARS',\n",
       " 'BBVA',\n",
       " 'JWN',\n",
       " 'HBI',\n",
       " 'FOXA',\n",
       " 'AEO',\n",
       " 'SKX',\n",
       " 'URBN',\n",
       " 'PAGS',\n",
       " 'NAVI',\n",
       " 'CMD',\n",
       " 'PENN',\n",
       " 'VERY',\n",
       " 'TVIX',\n",
       " 'ZIV',\n",
       " 'CBOE',\n",
       " 'PUMP',\n",
       " 'MOD',\n",
       " 'PBPB',\n",
       " 'PCB',\n",
       " 'ITCI',\n",
       " 'PSX',\n",
       " 'MFA',\n",
       " 'TAP',\n",
       " 'DEO',\n",
       " 'SEAS',\n",
       " 'TX',\n",
       " 'AVTR',\n",
       " 'JMIA',\n",
       " 'TLSA',\n",
       " 'HE',\n",
       " 'GRPN',\n",
       " 'SINO',\n",
       " 'HSY',\n",
       " 'XRAY',\n",
       " 'MRVL',\n",
       " 'RYTM',\n",
       " 'GPRO',\n",
       " 'ASX',\n",
       " 'EFT',\n",
       " 'CLB',\n",
       " 'ICE',\n",
       " 'NC',\n",
       " 'HA',\n",
       " 'HLT',\n",
       " 'IMAX',\n",
       " 'AR',\n",
       " 'KMX',\n",
       " 'AZO',\n",
       " 'IBEX',\n",
       " 'BGCP',\n",
       " 'INFO',\n",
       " 'MANU',\n",
       " 'UN',\n",
       " 'FDP',\n",
       " 'FLR',\n",
       " 'TOT',\n",
       " 'IBP',\n",
       " 'RILY',\n",
       " 'UNVR',\n",
       " 'PDD',\n",
       " 'VER',\n",
       " 'PLMR',\n",
       " 'SASR',\n",
       " 'OPRT',\n",
       " 'ZEAL',\n",
       " 'MRTX',\n",
       " 'RL',\n",
       " 'SCCO',\n",
       " 'WYNN',\n",
       " 'AIMC',\n",
       " 'BX',\n",
       " 'CLVS',\n",
       " 'CCO',\n",
       " 'CORT',\n",
       " 'ETM',\n",
       " 'GPN',\n",
       " 'GTN',\n",
       " 'IPG',\n",
       " 'KRYS',\n",
       " 'NBSE',\n",
       " 'NXST',\n",
       " 'OMC',\n",
       " 'SSP',\n",
       " 'SBBP',\n",
       " 'SBGI',\n",
       " 'TGNA',\n",
       " 'TXG',\n",
       " 'JBL',\n",
       " 'NEOG',\n",
       " 'CMTL',\n",
       " 'CTAS',\n",
       " 'NKE',\n",
       " 'SNX',\n",
       " 'DAVA',\n",
       " 'WOR',\n",
       " 'AIR',\n",
       " 'FUL',\n",
       " 'ACN',\n",
       " 'CAG',\n",
       " 'GDS',\n",
       " 'CAMP',\n",
       " 'MTN',\n",
       " 'PRGS',\n",
       " 'UEPS',\n",
       " 'ATOM',\n",
       " 'TC',\n",
       " 'CPA',\n",
       " 'CTV',\n",
       " 'QCOM',\n",
       " 'FVRR',\n",
       " 'FLIR',\n",
       " 'RELL',\n",
       " 'PLOW',\n",
       " 'OKE',\n",
       " 'ALLO',\n",
       " 'CMO',\n",
       " 'CR',\n",
       " 'PSC',\n",
       " 'COO',\n",
       " 'IDE',\n",
       " 'RE',\n",
       " 'FLY',\n",
       " 'BW',\n",
       " 'EH',\n",
       " 'NOK',\n",
       " 'ALB',\n",
       " 'ATNM',\n",
       " 'OPK',\n",
       " 'NAK',\n",
       " 'KT',\n",
       " 'PBR',\n",
       " 'ONVO',\n",
       " 'OI',\n",
       " 'RST',\n",
       " 'UL',\n",
       " 'CBNK',\n",
       " 'GRUB',\n",
       " 'PEG',\n",
       " 'ROK',\n",
       " 'TEAM',\n",
       " 'AC',\n",
       " 'VALE',\n",
       " 'BHP',\n",
       " 'SRG',\n",
       " 'VNO',\n",
       " 'CLDR',\n",
       " 'NET',\n",
       " 'AFYA',\n",
       " 'TY',\n",
       " 'AMG',\n",
       " 'CRMD',\n",
       " 'GGAL',\n",
       " 'BMA',\n",
       " 'SUPV',\n",
       " 'BBAR',\n",
       " 'BCH',\n",
       " 'SIRI',\n",
       " 'APRE',\n",
       " 'AUY',\n",
       " 'SWN',\n",
       " 'OKTA',\n",
       " 'DK',\n",
       " 'DVAX',\n",
       " 'NCR',\n",
       " 'FLEX',\n",
       " 'AVD',\n",
       " 'ECL',\n",
       " 'SD',\n",
       " 'TSN',\n",
       " 'COMM',\n",
       " 'FTC',\n",
       " 'RBC',\n",
       " 'CM',\n",
       " 'IEX',\n",
       " 'GENE',\n",
       " 'RAND',\n",
       " 'PRO',\n",
       " 'HPE',\n",
       " 'CARE',\n",
       " 'IVR',\n",
       " 'TDW',\n",
       " 'TYG',\n",
       " 'EAT',\n",
       " 'SAGE',\n",
       " 'RETA',\n",
       " 'BMRN',\n",
       " 'ALXN',\n",
       " 'REGN',\n",
       " 'RGNX',\n",
       " 'RARE',\n",
       " 'GSK',\n",
       " 'BMY',\n",
       " 'ABBV',\n",
       " 'AMGN',\n",
       " 'SGEN',\n",
       " 'FORD',\n",
       " 'LONE',\n",
       " 'SMH',\n",
       " 'FANG',\n",
       " 'BBC',\n",
       " 'ARCT',\n",
       " 'BNTX',\n",
       " 'HTBX',\n",
       " 'VXRT',\n",
       " 'BCS',\n",
       " 'INCY',\n",
       " 'MLM',\n",
       " 'MIC',\n",
       " 'MYGN',\n",
       " 'TRUP',\n",
       " 'PFD',\n",
       " 'TAK',\n",
       " 'MYL',\n",
       " 'TIF',\n",
       " 'HIBB',\n",
       " 'ORCL',\n",
       " 'CAH',\n",
       " 'NYT',\n",
       " 'DG',\n",
       " 'LEG',\n",
       " 'RUN',\n",
       " 'NATH',\n",
       " 'KIDS',\n",
       " 'TOPS',\n",
       " 'ABT',\n",
       " 'PRPL',\n",
       " 'CC',\n",
       " 'AMED',\n",
       " 'BKNG',\n",
       " 'SEE',\n",
       " 'HON',\n",
       " 'NR',\n",
       " 'CFR',\n",
       " 'WWE',\n",
       " 'ADI',\n",
       " 'PEY',\n",
       " 'ESPR',\n",
       " 'JAN',\n",
       " 'ETFC',\n",
       " 'HONE',\n",
       " 'EARN',\n",
       " 'CIO',\n",
       " 'HR',\n",
       " 'NRG',\n",
       " 'CP',\n",
       " 'LOPE',\n",
       " 'DLA',\n",
       " 'PTI',\n",
       " 'JD',\n",
       " 'VFC',\n",
       " 'AFL',\n",
       " 'ALK',\n",
       " 'ABEV',\n",
       " 'AON',\n",
       " 'AJG',\n",
       " 'ASB',\n",
       " 'ALV',\n",
       " 'AVT',\n",
       " 'BHE',\n",
       " 'CCMP',\n",
       " 'COG',\n",
       " 'COF',\n",
       " 'CLS',\n",
       " 'CERN',\n",
       " 'CHTR',\n",
       " 'CVA',\n",
       " 'CUBE',\n",
       " 'DECK',\n",
       " 'DLX',\n",
       " 'UFS',\n",
       " 'EMN',\n",
       " 'EHTH',\n",
       " 'FHB',\n",
       " 'FSLR',\n",
       " 'FTV',\n",
       " 'FWRD',\n",
       " 'GLPG',\n",
       " 'GBX',\n",
       " 'HLI',\n",
       " 'HUN',\n",
       " 'ITW',\n",
       " 'ILMN',\n",
       " 'TILE',\n",
       " 'JNPR',\n",
       " 'KEX',\n",
       " 'LEA',\n",
       " 'LOGM',\n",
       " 'LPL',\n",
       " 'LPLA',\n",
       " 'MXL',\n",
       " 'MHK',\n",
       " 'OSIS',\n",
       " 'PEB',\n",
       " 'POWI',\n",
       " 'PFG',\n",
       " 'PFPT',\n",
       " 'PROS',\n",
       " 'RMD',\n",
       " 'TNET',\n",
       " 'UHS',\n",
       " 'VRSN',\n",
       " 'VCRA',\n",
       " 'VLRS',\n",
       " 'WRE',\n",
       " 'WERN',\n",
       " 'WY',\n",
       " 'WPP',\n",
       " 'FTI',\n",
       " 'FCN',\n",
       " 'KKR',\n",
       " 'KREF',\n",
       " 'CVS',\n",
       " 'FFIC',\n",
       " 'ARCE',\n",
       " 'BYSI',\n",
       " 'MD',\n",
       " 'TXMD',\n",
       " 'CABA',\n",
       " 'PGNY',\n",
       " 'TFFP',\n",
       " 'DAO',\n",
       " 'ADS',\n",
       " 'ACRS',\n",
       " 'SENS',\n",
       " 'BOOM',\n",
       " 'ANIK',\n",
       " 'CSLT',\n",
       " 'SPSC',\n",
       " 'PFNX',\n",
       " 'FET',\n",
       " 'MNKD',\n",
       " 'YNDX',\n",
       " 'HEXO',\n",
       " 'ENVA',\n",
       " 'SMSI',\n",
       " 'DRQ',\n",
       " 'NRZ',\n",
       " 'OMCL',\n",
       " 'SBCF',\n",
       " 'AXTA',\n",
       " 'HCM',\n",
       " 'CDAY',\n",
       " 'ED',\n",
       " 'DKS',\n",
       " 'SMG',\n",
       " 'FSB',\n",
       " 'GPC',\n",
       " 'HBAN',\n",
       " 'ODFL',\n",
       " 'PRLB',\n",
       " 'PLAN',\n",
       " 'COUP',\n",
       " 'ESSA',\n",
       " 'EPIX',\n",
       " 'GLPI',\n",
       " 'HRL',\n",
       " 'MGP',\n",
       " 'PAYC',\n",
       " 'PCTY',\n",
       " 'PPC',\n",
       " 'SAFM',\n",
       " 'WPC',\n",
       " 'PETS',\n",
       " 'ACC',\n",
       " 'BXS',\n",
       " 'CDNS',\n",
       " 'CE',\n",
       " 'ELS',\n",
       " 'HXL',\n",
       " 'HMST',\n",
       " 'RNST',\n",
       " 'TACO',\n",
       " 'ZION',\n",
       " 'ABG',\n",
       " 'GATX',\n",
       " 'JBLU',\n",
       " 'NUE',\n",
       " 'NVS',\n",
       " 'PCAR',\n",
       " 'PII',\n",
       " 'RF',\n",
       " 'SHW',\n",
       " 'SNV',\n",
       " 'CNI',\n",
       " 'CSL',\n",
       " 'IRBT',\n",
       " 'MANH',\n",
       " 'MPWR',\n",
       " 'MTH',\n",
       " 'TER',\n",
       " 'TXN',\n",
       " 'WHR',\n",
       " 'ABB',\n",
       " 'AVY',\n",
       " 'CLF',\n",
       " 'CMC',\n",
       " 'GWW',\n",
       " 'HRI',\n",
       " 'KNX',\n",
       " 'LAD',\n",
       " 'MHO',\n",
       " 'NSC',\n",
       " 'NTRS',\n",
       " 'BPOP',\n",
       " 'RCI',\n",
       " 'ROL',\n",
       " 'SLGN',\n",
       " 'SLAB',\n",
       " 'TDY',\n",
       " 'TMO',\n",
       " 'WM',\n",
       " 'WGO',\n",
       " 'AEM',\n",
       " 'ASGN',\n",
       " 'BCOV',\n",
       " 'CLGX',\n",
       " 'EBAY',\n",
       " 'ECHO',\n",
       " 'EW',\n",
       " 'EFX',\n",
       " 'FFIV',\n",
       " 'KNL',\n",
       " 'KRA',\n",
       " 'LSTR',\n",
       " 'LMAT',\n",
       " 'NTGR',\n",
       " 'ORLY',\n",
       " 'PKG',\n",
       " 'PYPL',\n",
       " 'SAVE',\n",
       " 'VAR',\n",
       " 'VMI',\n",
       " 'AB',\n",
       " 'BAX',\n",
       " 'CRS',\n",
       " 'CTXS',\n",
       " 'COWN',\n",
       " 'DHR',\n",
       " 'GPI',\n",
       " 'KIM',\n",
       " 'LH',\n",
       " 'MNRO',\n",
       " 'MSM',\n",
       " 'PTEN',\n",
       " 'ROP',\n",
       " 'RS',\n",
       " 'STM',\n",
       " 'SWK',\n",
       " 'TSCO',\n",
       " 'VLO',\n",
       " 'GRA',\n",
       " 'WST',\n",
       " 'BJRI',\n",
       " 'ELY',\n",
       " 'VTR',\n",
       " 'WETF',\n",
       " 'VG',\n",
       " 'NTR',\n",
       " 'AA',\n",
       " 'GSX',\n",
       " 'YELP',\n",
       " 'PING',\n",
       " 'RBS',\n",
       " 'MMS',\n",
       " 'EYPT',\n",
       " 'ALX',\n",
       " 'IEP',\n",
       " 'ETH',\n",
       " 'KGC',\n",
       " 'NEM',\n",
       " 'EGO',\n",
       " 'SNE',\n",
       " 'ESG',\n",
       " 'UTF',\n",
       " 'SLB',\n",
       " 'ELSE',\n",
       " 'MDB',\n",
       " 'NAV',\n",
       " 'HERO',\n",
       " 'IBB',\n",
       " 'MKTX',\n",
       " 'HI',\n",
       " 'WDAY',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(foundsymbols))\n",
    "with open(\"foundsymbols.txt\",\"w\") as fh:\n",
    "    fh.write(\"\\n\".join(foundsymbols))\n",
    "foundsymbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34504 27603\n"
     ]
    }
   ],
   "source": [
    "#split into multiple corpus here\n",
    "\n",
    "\n",
    "import random\n",
    "random.shuffle(relevant_posts)\n",
    "n_train = int(0.8 * len(relevant_posts))\n",
    "\n",
    "training_posts = relevant_posts[:n_train]\n",
    "test_posts = relevant_posts[n_train:]\n",
    "val_posts = test_posts[:n_train//2]\n",
    "test_posts = test_posts[n_train//2:]\n",
    "\n",
    "vocab_input = [t[\"data\"][\"title\"]+\" \"+t[\"data\"][\"selftext\"] for t in training_posts]\n",
    "print(len(relevant_posts),n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27603 13801\n",
      "6901\n",
      "3450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3451"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_posts = relevant_posts[n_train:]\n",
    "print(n_train,n_train//2)\n",
    "test_posts = relevant_posts[n_train:]\n",
    "print(len(test_posts))\n",
    "print(len(test_posts)//2)\n",
    "val_posts = test_posts[:len(test_posts)//2]\n",
    "test_posts = test_posts[len(test_posts)//2:]\n",
    "len(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vocab_size=2500\n",
    "\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english', tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "\n",
    "vectors = vectorizer.fit_transform(vocab_input)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "idx = np.arange(vectors.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "vectors = vectors[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold=.05\n",
    "lt = LemmaTokenizer()\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def calculateData(post):\n",
    "    #print(post)\n",
    "    data=post[\"data\"]\n",
    "    thesesymbols=post[\"symbols\"]\n",
    "    text = data[\"title\"]+\" \"+data[\"selftext\"]\n",
    "    texttokens=lt(text)\n",
    "    #print(text,texttokens)\n",
    "    #print(\"Symbol:\",thesesymbols)\n",
    "    wordvector=[]\n",
    "    for word in vocab_list:\n",
    "        wordvector.append(texttokens.count(word))\n",
    "    returnarray=[]\n",
    "    for symbolmap in thesesymbols:\n",
    "        symbol=list(symbolmap.keys())[0]\n",
    "        try:\n",
    "        #if True:\n",
    "            \n",
    "                #if word in texttokens:\n",
    "                    #print(word)\n",
    "                #    wordvector.append(1)\n",
    "                #else:\n",
    "                #    wordvector.append(0)\n",
    "                #locword = text.find(word)\n",
    "                #distance=0\n",
    "                #if locword>-1:\n",
    "                #    distance=abs(text.find(symbolmap[symbol])-locword)\n",
    "                #    print(symbol,word,distance)\n",
    "                #wordvector.append(distance)\n",
    "            #print(symbol,wordvector)\n",
    "            posttime=[0,0,0,0]\n",
    "            start=ET.localize(datetime.datetime.fromtimestamp(data[\"created_utc\"]))\n",
    "            if start.hour>=21:\n",
    "                posttime[0]=1\n",
    "            elif start.hour < 5:\n",
    "                posttime[0]=1\n",
    "            elif start.hour < 9:\n",
    "                posttime[1]=1\n",
    "            elif start.hour < 16:\n",
    "                posttime[2]=1\n",
    "            else:\n",
    "                posttime[3]=1\n",
    "            if start.hour>=9:\n",
    "                start=start+datetime.timedelta(days=1)\n",
    "            start=start.replace(hour=9,minute=0,second=0,microsecond=0).date()\n",
    "            end=start+dt\n",
    "            startprice=getstockprice(symbol,start)\n",
    "            endprice=getstockprice(symbol,end)\n",
    "            #print(start,end,history)\n",
    "            delta=(endprice-startprice)/startprice\n",
    "            if delta<=-1*(threshhold):\n",
    "                result=1\n",
    "            elif delta>=threshhold:\n",
    "                result=2\n",
    "                #print(symbol,delta,result,wordvector)\n",
    "            else:\n",
    "                result=0\n",
    "            returnarray.append([result]+posttime+wordvector)\n",
    "        except Exception as err:\n",
    "            print(\"Error with\",symbol,\", skipping this one:\",err)\n",
    "    return returnarray\n",
    "\n",
    "\n",
    "def calculateVectors(posts):\n",
    "    vector = []\n",
    "    i=0\n",
    "    for post in posts:\n",
    "        #try:\n",
    "        if True:\n",
    "           response=calculateData(post)\n",
    "           for row in response:\n",
    "                vector.append(row)\n",
    "        #except Exception as err:\n",
    "        #    print(\"Error\",err)\n",
    "        #    pass\n",
    "        i+=1\n",
    "        if(round(i/100,0)==i/100):\n",
    "            update_progress(i / len(posts))\n",
    "    vector=np.array(vector).astype('float32')\n",
    "    return vector\n",
    "\n",
    "training_vector = calculateVectors(training_posts)\n",
    "test_vector = calculateVectors(test_posts)\n",
    "val_vector = calculateVectors(val_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(training_vector, columns=[\"Output\"]+[\"21-4\",\"5-8\",\"9-15\",\"16-20\"]+vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_vector[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "prefix='wsb-xgboost'\n",
    "region = boto3.Session().region_name\n",
    " \n",
    "def to_libsvm(f, labels, values):\n",
    "     f.write(bytes('\\n'.join(\n",
    "         ['{} {}'.format(label, ' '.join(['{}:{}'.format(i + 1, el) for i, el in enumerate(vec)])) for label, vec in\n",
    "          zip(labels, values)]), 'utf-8'))\n",
    "     return f\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session(region_name=region).resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "    labels = [t.tolist() for t in partition[:,0]]\n",
    "    vectors = [t.tolist() for t in partition[:,1:]]\n",
    "    num_partition = 5                                 # partition file into 5 parts\n",
    "    partition_bound = int(len(labels)/num_partition)\n",
    "    for i in range(num_partition):\n",
    "        f = io.BytesIO()\n",
    "        to_libsvm(f, labels[i*partition_bound:(i+1)*partition_bound], vectors[i*partition_bound:(i+1)*partition_bound])\n",
    "        f.seek(0)\n",
    "        key = \"{}/{}/examples{}\".format(prefix,partition_name,str(i))\n",
    "        url = 's3n://{}/{}'.format(bucket, key)\n",
    "        print('Writing to {}'.format(url))\n",
    "        write_to_s3(f, bucket, key)\n",
    "        print('Done writing to {}'.format(url))\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = \"{}/{}/examples{}\".format(prefix,partition_name, number)\n",
    "    url = 's3n://{}/{}'.format(bucket, key)\n",
    "    print('Reading from {}'.format(url))\n",
    "    s3 = boto3.resource('s3', region_name = region)\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    #try:\n",
    "    #    s3.Bucket(bucket).download_file(key, 'mnist.local.test')\n",
    "    #except botocore.exceptions.ClientError as e:\n",
    "    #    if e.response['Error']['Code'] == \"404\":\n",
    "    #        print('The object does not exist at {}.'.format(url))\n",
    "    #    else:\n",
    "    #        raise        \n",
    "        \n",
    "def convert_data():\n",
    "    partitions = [('train', training_vector), ('validation', val_vector), ('test', test_vector)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print('{}: {} {}'.format(partition_name, partition[:,0].shape, partition[:,1:].shape))\n",
    "        upload_to_s3(partition_name, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vector[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "convert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"eta\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"alpha\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"min_child_weight\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"max_depth\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 20,\n",
    "      \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:merror\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": container,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/train/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\" \n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/validation/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/{}/xgboost\".format(bucket,prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 2,   \n",
    "        \"InstanceType\": \"ml.m4.10xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"eval_metric\": \"merror\",\n",
    "      \"num_round\": \"100\",\n",
    "      \"objective\": \"multi:softmax\",\n",
    "      \"num_class\": \"3\",\n",
    "      \"rate_drop\": \"0.3\",\n",
    "      \"tweedie_variance_power\": \"1.4\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 43200\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "sm = boto3.Session(region_name=region).client('sagemaker')\n",
    "\n",
    "tuning_job_name = \"MyTuningJob\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sm.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                           HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                           TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Ensure that the train and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "common_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": bucket_path + \"/\"+ prefix + \"/xgboost\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": \"ml.m4.10xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": \"3\",\n",
    "        \"num_round\": \"30\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/train/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\" \n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/validation/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "#single machine job params\n",
    "single_machine_job_name = 'wsb-xgboost-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Job name is:\", single_machine_job_name)\n",
    "\n",
    "single_machine_job_params = copy.deepcopy(common_training_params)\n",
    "single_machine_job_params['TrainingJobName'] = single_machine_job_name\n",
    "single_machine_job_params['OutputDataConfig']['S3OutputPath'] = bucket_path + \"/\"+ prefix + \"/xgboost-single\"\n",
    "single_machine_job_params['ResourceConfig']['InstanceCount'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "sm.create_training_job(**single_machine_job_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=single_machine_job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=single_machine_job_name)\n",
    "status = sm.describe_training_job(TrainingJobName=single_machine_job_name)['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "if status == 'Failed':\n",
    "    message = sm.describe_training_job(TrainingJobName=single_machine_job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=single_machine_job_name + '-mod'\n",
    "print(model_name)\n",
    "\n",
    "info = sm.describe_training_job(TrainingJobName=single_machine_job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'DEMO-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_s3('test', 0, 'wsb.local.test') # reading the first part file within test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 wsb.local.test > wsb.single.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = 'wsb.single.test' #customize to your test file 'mnist.single.test' if use the data above\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/x-libsvm', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read().decode('ascii')\n",
    "print('Predicted label is {}.'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType=content_type, \n",
    "                                   Body=payload)\n",
    "    result = response['Body'].read().decode('ascii')\n",
    "    preds = [float(num) for num in result.split(',')]\n",
    "    return preds\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    for offset in range(0, items, batch_size):\n",
    "        arrs.extend(do_predict(data[offset:min(offset+batch_size, items)], endpoint_name, content_type))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = 'wsb.local.test'\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [float(line.split(' ')[0]) for line in payload.split('\\n')]\n",
    "test_data = payload.split('\\n')\n",
    "preds = batch_predict(test_data, 100, endpoint_name, 'text/x-libsvm')\n",
    "\n",
    "print ('\\nerror rate=%f' % ( sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = numpy.sum(predictions == labels)\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = numpy.zeros([3,3], numpy.int32)\n",
    "    bundled = zip(predictions, labels)\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[int(predicted), int(actual)] += 1\n",
    "    \n",
    "    return error, confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "NUM_LABELS = 3  # change it according to num_class in your dataset\n",
    "test_error, confusions = error_rate(numpy.asarray(preds), numpy.asarray(labels))\n",
    "print('Test error: %.1f%%' % test_error)\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(False)\n",
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.yticks(numpy.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');\n",
    "\n",
    "for i, cas in enumerate(confusions):\n",
    "    for j, count in enumerate(cas):\n",
    "        if count > 0:\n",
    "            xoff = .07 * len(str(count))\n",
    "            plt.text(j-xoff, i+.2, int(count), fontsize=9, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.classification_report(numpy.asarray(labels),numpy.asarray(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
