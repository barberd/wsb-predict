{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://{bucket}/wsb/data/ data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET = pytz.timezone('US/Eastern')\n",
    "\n",
    "ignorelist=[\"DD\",\"FREE\",\"CASH\",\"ON\",\"I\"]\n",
    "allsymbols=[]\n",
    "with open(\"allsymbols.txt\") as fh:\n",
    "    allsymbols=fh.readlines()\n",
    "allsymbols = [x.strip() for x in allsymbols]\n",
    "\n",
    "dt=datetime.timedelta(days=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "stockdata={}\n",
    "def getstockprice(symbol,date):\n",
    "    if symbol not in stockdata:\n",
    "        with open(\"stockprices/\"+symbol,\"r\") as fh:\n",
    "            data=json.load(fh)\n",
    "        stockdata[symbol]={}\n",
    "        for stockday in data:\n",
    "            indate=datetime.datetime.strptime(stockday[\"begins_at\"],\"%Y-%m-%dT%H:%M:%SZ\").date()\n",
    "            stockdata[symbol][indate]=stockday\n",
    "    today=datetime.date.today()\n",
    "    while True:\n",
    "        #print(date)\n",
    "        if date in stockdata[symbol]:\n",
    "            return float(stockdata[symbol][date]['close_price'])\n",
    "        date=date+datetime.timedelta(days=1)\n",
    "        if date>today:\n",
    "            break\n",
    "    raise Exception(\"No stock data found\")\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"data/\")\n",
    "\n",
    "foundsymbols=[]\n",
    "relevant_posts = []\n",
    "i=0\n",
    "for file in files:\n",
    "  if(round(i/100,0)==i/100):\n",
    "        update_progress(i / len(files))  \n",
    "  with open(\"data/\"+file,\"r\") as fh:\n",
    "    try:\n",
    "        data=json.load(fh)\n",
    "        text = (data[\"title\"]+\" \"+data[\"selftext\"]).replace('\\\"','').replace('\\'','')\n",
    "        allmatches=[]\n",
    "        matches=re.findall('\\W*([A-Z][A-Z\\.]{0,3})\\W',text)\n",
    "        for submatch in matches:\n",
    "            allmatches.append({submatch:submatch})\n",
    "            #pass\n",
    "        matches2=re.findall('\\W*(\\$[a-z\\.]{1,4})',text)\n",
    "        for submatch in matches2:\n",
    "            allmatches.append({submatch.upper()[1:]:submatch})\n",
    "        thesesymbols=[]\n",
    "        #print(allmatches)\n",
    "        for submatch in allmatches:\n",
    "            symbol=list(submatch.keys())[0]\n",
    "            #print(submatch.keys())\n",
    "            #print(symbol)\n",
    "            #print([list(x.keys())[0] for x in thesesymbols])\n",
    "            if symbol in allsymbols and symbol not in [list(x.keys())[0] for x in thesesymbols] and symbol not in ignorelist:\n",
    "                thesesymbols.append(submatch)\n",
    "                if symbol not in foundsymbols:\n",
    "                    foundsymbols.append(symbol)\n",
    "        if len(thesesymbols)>0:\n",
    "            relevant_posts.append({\"data\":data,\"symbols\":thesesymbols})\n",
    "            #print(thesesymbols)\n",
    "    except Exception as err:\n",
    "      print(\"Error with\",file)\n",
    "      raise(err)\n",
    "  i+=1\n",
    "    \n",
    "\n",
    "  #if i>1000:\n",
    "  #  break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(foundsymbols))\n",
    "with open(\"foundsymbols.txt\",\"w\") as fh:\n",
    "    fh.write(\"\\n\".join(foundsymbols))\n",
    "foundsymbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into multiple corpus here\n",
    "\n",
    "\n",
    "import random\n",
    "random.shuffle(relevant_posts)\n",
    "n_train = int(0.8 * len(relevant_posts))\n",
    "\n",
    "training_posts = relevant_posts[:n_train]\n",
    "test_posts = relevant_posts[n_train:]\n",
    "val_posts = test_posts[:n_train//2]\n",
    "test_posts = test_posts[n_train//2:]\n",
    "\n",
    "vocab_input = [t[\"data\"][\"title\"]+\" \"+t[\"data\"][\"selftext\"] for t in training_posts]\n",
    "print(len(relevant_posts),n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posts = relevant_posts[n_train:]\n",
    "print(n_train,n_train//2)\n",
    "test_posts = relevant_posts[n_train:]\n",
    "print(len(test_posts))\n",
    "print(len(test_posts)//2)\n",
    "val_posts = test_posts[:len(test_posts)//2]\n",
    "test_posts = test_posts[len(test_posts)//2:]\n",
    "len(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=\"\"\n",
    "for post in training_posts:\n",
    "    data=post[\"data\"]\n",
    "    thesesymbols=post[\"symbols\"]\n",
    "    text = data[\"title\"]+\" \"+data[\"selftext\"] \n",
    "    training_data += text.replace(\"\\n\",\"\")+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "prefix='wsb-xgboost-blazingtext'\n",
    "region = boto3.Session().region_name\n",
    "s3=boto3.client('s3')\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "\n",
    "s3.put_object(Bucket=bucket,Key=train_channel,Body=training_data)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_input = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m4.10xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bt_model.set_hyperparameters(mode=\"batch_skipgram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_input = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold=.05\n",
    "\n",
    "def calculateData(post):\n",
    "    #print(post)\n",
    "    data=post[\"data\"]\n",
    "    thesesymbols=post[\"symbols\"]\n",
    "    text = data[\"title\"]+\" \"+data[\"selftext\"]\n",
    "    texttokens=lt(text)\n",
    "    #print(text,texttokens)\n",
    "    #print(\"Symbol:\",thesesymbols)\n",
    "    payload = {\"instances\" : text.split(\" \")}\n",
    "    response = text_classifier.predict(json.dumps(payload))\n",
    "    wordvector=json.loads(response)\n",
    "    returnarray=[]\n",
    "    for symbolmap in thesesymbols:\n",
    "        symbol=list(symbolmap.keys())[0]\n",
    "        try:\n",
    "        #if True:\n",
    "            \n",
    "                #if word in texttokens:\n",
    "                    #print(word)\n",
    "                #    wordvector.append(1)\n",
    "                #else:\n",
    "                #    wordvector.append(0)\n",
    "                #locword = text.find(word)\n",
    "                #distance=0\n",
    "                #if locword>-1:\n",
    "                #    distance=abs(text.find(symbolmap[symbol])-locword)\n",
    "                #    print(symbol,word,distance)\n",
    "                #wordvector.append(distance)\n",
    "            #print(symbol,wordvector)\n",
    "            posttime=[0,0,0,0]\n",
    "            start=ET.localize(datetime.datetime.fromtimestamp(data[\"created_utc\"]))\n",
    "            if start.hour>=21:\n",
    "                posttime[0]=1\n",
    "            elif start.hour < 5:\n",
    "                posttime[0]=1\n",
    "            elif start.hour < 9:\n",
    "                posttime[1]=1\n",
    "            elif start.hour < 16:\n",
    "                posttime[2]=1\n",
    "            else:\n",
    "                posttime[3]=1\n",
    "            if start.hour>=9:\n",
    "                start=start+datetime.timedelta(days=1)\n",
    "            start=start.replace(hour=9,minute=0,second=0,microsecond=0).date()\n",
    "            end=start+dt\n",
    "            startprice=getstockprice(symbol,start)\n",
    "            endprice=getstockprice(symbol,end)\n",
    "            #print(start,end,history)\n",
    "            delta=(endprice-startprice)/startprice\n",
    "            if delta<=-1*(threshhold):\n",
    "                result=1\n",
    "            elif delta>=threshhold:\n",
    "                result=2\n",
    "                #print(symbol,delta,result,wordvector)\n",
    "            else:\n",
    "                result=0\n",
    "            returnarray.append([result]+posttime+wordvector)\n",
    "        except Exception as err:\n",
    "            print(\"Error with\",symbol,\", skipping this one:\",err)\n",
    "    return returnarray\n",
    "\n",
    "\n",
    "def calculateVectors(posts):\n",
    "    vector = []\n",
    "    i=0\n",
    "    for post in posts:\n",
    "        #try:\n",
    "        if True:\n",
    "           response=calculateData(post)\n",
    "           for row in response:\n",
    "                vector.append(row)\n",
    "        #except Exception as err:\n",
    "        #    print(\"Error\",err)\n",
    "        #    pass\n",
    "        i+=1\n",
    "        if(round(i/100,0)==i/100):\n",
    "            update_progress(i / len(posts))\n",
    "    vector=np.array(vector).astype('float32')\n",
    "    return vector\n",
    "\n",
    "training_vector = calculateVectors(training_posts)\n",
    "test_vector = calculateVectors(test_posts)\n",
    "val_vector = calculateVectors(val_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Output</th>\n",
       "      <th>21-4</th>\n",
       "      <th>5-8</th>\n",
       "      <th>9-15</th>\n",
       "      <th>16-20</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>yolos</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yoy</th>\n",
       "      <th>yr/yr</th>\n",
       "      <th>ytd</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68391</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68392</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68393</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68394</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68395</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68396 rows × 2505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Output  21-4  5-8  9-15  16-20  ability  able  absolute  absolutely  \\\n",
       "0         0.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "1         2.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "2         0.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "3         0.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "4         0.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "...       ...   ...  ...   ...    ...      ...   ...       ...         ...   \n",
       "68391     1.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "68392     1.0   0.0  0.0   1.0    0.0      0.0   0.0       1.0         0.0   \n",
       "68393     0.0   0.0  0.0   1.0    0.0      0.0   0.0       1.0         0.0   \n",
       "68394     0.0   0.0  0.0   1.0    0.0      0.0   0.0       1.0         0.0   \n",
       "68395     1.0   1.0  0.0   0.0    0.0      0.0   0.0       0.0         0.0   \n",
       "\n",
       "       accept  ...  yolos  york  young  youtube  yoy  yr/yr  ytd  zero  zone  \\\n",
       "0         0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "1         0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "2         0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "3         0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "4         0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "...       ...  ...    ...   ...    ...      ...  ...    ...  ...   ...   ...   \n",
       "68391     0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "68392     0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "68393     0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "68394     0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "68395     0.0  ...    0.0   0.0    0.0      0.0  0.0    0.0  0.0   0.0   0.0   \n",
       "\n",
       "       zoom  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "...     ...  \n",
       "68391   0.0  \n",
       "68392   0.0  \n",
       "68393   0.0  \n",
       "68394   0.0  \n",
       "68395   0.0  \n",
       "\n",
       "[68396 rows x 2505 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(training_vector, columns=[\"Output\"]+[\"21-4\",\"5-8\",\"9-15\",\"16-20\"]+vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vector[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.7 ms, sys: 3.98 ms, total: 9.68 ms\n",
      "Wall time: 9.19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "prefix='wsb-xgboost'\n",
    "region = boto3.Session().region_name\n",
    " \n",
    "def to_libsvm(f, labels, values):\n",
    "     f.write(bytes('\\n'.join(\n",
    "         ['{} {}'.format(label, ' '.join(['{}:{}'.format(i + 1, el) for i, el in enumerate(vec)])) for label, vec in\n",
    "          zip(labels, values)]), 'utf-8'))\n",
    "     return f\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session(region_name=region).resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "    labels = [t.tolist() for t in partition[:,0]]\n",
    "    vectors = [t.tolist() for t in partition[:,1:]]\n",
    "    num_partition = 5                                 # partition file into 5 parts\n",
    "    partition_bound = int(len(labels)/num_partition)\n",
    "    for i in range(num_partition):\n",
    "        f = io.BytesIO()\n",
    "        to_libsvm(f, labels[i*partition_bound:(i+1)*partition_bound], vectors[i*partition_bound:(i+1)*partition_bound])\n",
    "        f.seek(0)\n",
    "        key = \"{}/{}/examples{}\".format(prefix,partition_name,str(i))\n",
    "        url = 's3n://{}/{}'.format(bucket, key)\n",
    "        print('Writing to {}'.format(url))\n",
    "        write_to_s3(f, bucket, key)\n",
    "        print('Done writing to {}'.format(url))\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = \"{}/{}/examples{}\".format(prefix,partition_name, number)\n",
    "    url = 's3n://{}/{}'.format(bucket, key)\n",
    "    print('Reading from {}'.format(url))\n",
    "    s3 = boto3.resource('s3', region_name = region)\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    #try:\n",
    "    #    s3.Bucket(bucket).download_file(key, 'mnist.local.test')\n",
    "    #except botocore.exceptions.ClientError as e:\n",
    "    #    if e.response['Error']['Code'] == \"404\":\n",
    "    #        print('The object does not exist at {}.'.format(url))\n",
    "    #    else:\n",
    "    #        raise        \n",
    "        \n",
    "def convert_data():\n",
    "    partitions = [('train', training_vector), ('validation', val_vector), ('test', test_vector)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print('{}: {} {}'.format(partition_name, partition[:,0].shape, partition[:,1:].shape))\n",
    "        upload_to_s3(partition_name, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8285, 2504)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vector[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (68396,) (68396, 2504)\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples0\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples0\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples1\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples1\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples2\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples2\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples3\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples3\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples4\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/train/examples4\n",
      "validation: (8285,) (8285, 2504)\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples0\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples0\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples1\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples1\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples2\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples2\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples3\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples3\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples4\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/validation/examples4\n",
      "test: (9309,) (9309, 2504)\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples0\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples0\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples1\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples1\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples2\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples2\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples3\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples3\n",
      "Writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples4\n",
      "Done writing to s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples4\n",
      "CPU times: user 1min 52s, sys: 13 s, total: 2min 5s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "convert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"eta\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"alpha\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"min_child_weight\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"max_depth\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 20,\n",
    "      \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:merror\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": container,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/train/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\" \n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/validation/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/{}/xgboost\".format(bucket,prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 2,   \n",
    "        \"InstanceType\": \"ml.m4.10xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"eval_metric\": \"merror\",\n",
    "      \"num_round\": \"100\",\n",
    "      \"objective\": \"multi:softmax\",\n",
    "      \"num_class\": \"3\",\n",
    "      \"rate_drop\": \"0.3\",\n",
    "      \"tweedie_variance_power\": \"1.4\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 43200\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:011113936377:hyper-parameter-tuning-job/mytuningjob2020-08-11-11-05-30',\n",
       " 'ResponseMetadata': {'RequestId': '3d684515-55f3-4fa2-9cac-f779d1e00431',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3d684515-55f3-4fa2-9cac-f779d1e00431',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '131',\n",
       "   'date': 'Tue, 11 Aug 2020 11:05:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "sm = boto3.Session(region_name=region).client('sagemaker')\n",
    "\n",
    "tuning_job_name = \"MyTuningJob\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sm.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                           HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                           TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Ensure that the train and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "common_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": bucket_path + \"/\"+ prefix + \"/xgboost\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": \"ml.m4.10xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": \"3\",\n",
    "        \"num_round\": \"30\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/train/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\" \n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/validation/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name is: wsb-xgboost-classification2020-08-11-11-05-43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "#single machine job params\n",
    "single_machine_job_name = 'wsb-xgboost-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Job name is:\", single_machine_job_name)\n",
    "\n",
    "single_machine_job_params = copy.deepcopy(common_training_params)\n",
    "single_machine_job_params['TrainingJobName'] = single_machine_job_name\n",
    "single_machine_job_params['OutputDataConfig']['S3OutputPath'] = bucket_path + \"/\"+ prefix + \"/xgboost-single\"\n",
    "single_machine_job_params['ResourceConfig']['InstanceCount'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress\n",
      "Training job ended with status: Completed\n",
      "CPU times: user 85.9 ms, sys: 8.06 ms, total: 94 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "sm.create_training_job(**single_machine_job_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=single_machine_job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=single_machine_job_name)\n",
    "status = sm.describe_training_job(TrainingJobName=single_machine_job_name)['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "if status == 'Failed':\n",
    "    message = sm.describe_training_job(TrainingJobName=single_machine_job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsb-xgboost-classification2020-08-11-11-05-43-mod\n",
      "https://s3-us-east-1.amazonaws.com/sagemaker-us-east-1-011113936377/wsb-xgboost/xgboost-single/wsb-xgboost-classification2020-08-11-11-05-43/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-1:011113936377:model/wsb-xgboost-classification2020-08-11-11-05-43-mod\n",
      "CPU times: user 7.03 ms, sys: 51 µs, total: 7.08 ms\n",
      "Wall time: 423 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=single_machine_job_name + '-mod'\n",
    "print(model_name)\n",
    "\n",
    "info = sm.describe_training_job(TrainingJobName=single_machine_job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-XGBoostEndpointConfig-2020-08-11-11-11-49\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:011113936377:endpoint-config/demo-xgboostendpointconfig-2020-08-11-11-11-49\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'DEMO-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-XGBoostEndpoint-2020-08-11-11-11-49\n",
      "arn:aws:sagemaker:us-east-1:011113936377:endpoint/demo-xgboostendpoint-2020-08-11-11-11-49\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:011113936377:endpoint/demo-xgboostendpoint-2020-08-11-11-11-49\n",
      "Status: InService\n",
      "CPU times: user 142 ms, sys: 229 µs, total: 142 ms\n",
      "Wall time: 9min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from s3n://sagemaker-us-east-1-011113936377/wsb-xgboost/test/examples0\n"
     ]
    }
   ],
   "source": [
    "download_from_s3('test', 0, 'wsb.local.test') # reading the first part file within test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 wsb.local.test > wsb.single.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label is 0.0.\n",
      "CPU times: user 8.38 ms, sys: 7.97 ms, total: 16.4 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = 'wsb.single.test' #customize to your test file 'mnist.single.test' if use the data above\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/x-libsvm', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read().decode('ascii')\n",
    "print('Predicted label is {}.'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType=content_type, \n",
    "                                   Body=payload)\n",
    "    result = response['Body'].read().decode('ascii')\n",
    "    preds = [float(num) for num in result.split(',')]\n",
    "    return preds\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    for offset in range(0, items, batch_size):\n",
    "        arrs.extend(do_predict(data[offset:min(offset+batch_size, items)], endpoint_name, content_type))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................\n",
      "error rate=0.448684\n",
      "CPU times: user 467 ms, sys: 119 ms, total: 585 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = 'wsb.local.test'\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [float(line.split(' ')[0]) for line in payload.split('\\n')]\n",
    "test_data = payload.split('\\n')\n",
    "preds = batch_predict(test_data, 100, endpoint_name, 'text/x-libsvm')\n",
    "\n",
    "print ('\\nerror rate=%f' % ( sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = numpy.sum(predictions == labels)\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = numpy.zeros([3,3], numpy.int32)\n",
    "    bundled = zip(predictions, labels)\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[int(predicted), int(actual)] += 1\n",
    "    \n",
    "    return error, confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 44.9%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "NUM_LABELS = 3  # change it according to num_class in your dataset\n",
    "test_error, confusions = error_rate(numpy.asarray(preds), numpy.asarray(labels))\n",
    "print('Test error: %.1f%%' % test_error)\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(False)\n",
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.yticks(numpy.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');\n",
    "\n",
    "for i, cas in enumerate(confusions):\n",
    "    for j, count in enumerate(cas):\n",
    "        if count > 0:\n",
    "            xoff = .07 * len(str(count))\n",
    "            plt.text(j-xoff, i+.2, int(count), fontsize=9, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '71d86b6d-1e11-464c-bb0a-1868d91872e0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '71d86b6d-1e11-464c-bb0a-1868d91872e0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 11 Aug 2020 11:23:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.98      0.71      1024\n",
      "         1.0       0.39      0.04      0.07       487\n",
      "         2.0       0.00      0.00      0.00       350\n",
      "\n",
      "    accuracy                           0.55      1861\n",
      "   macro avg       0.31      0.34      0.26      1861\n",
      "weighted avg       0.41      0.55      0.41      1861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.classification_report(numpy.asarray(labels),numpy.asarray(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
